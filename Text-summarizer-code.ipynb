{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import summarize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from gensim.models import Word2Vec\n",
    "import skipthoughts\n",
    "import os\n",
    "\n",
    "import theano\n",
    "import theano.tensor as tensor\n",
    "\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import copy\n",
    "import nltk\n",
    "\n",
    "from collections import OrderedDict, defaultdict\n",
    "from scipy.linalg import norm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Splits the text into individual sentences\n",
    "    \"\"\"\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return sentences   \n",
    "        \n",
    "def skipthought_encode(sentences):\n",
    "    \"\"\"\n",
    "    Obtains sentence embeddings for each sentence in the text\n",
    "    \"\"\"\n",
    "    print('Loading pre-trained models...')\n",
    "    model = skipthoughts.load_model()\n",
    "    encoder = skipthoughts.Encoder(model)\n",
    "    print('Encoding sentences...')\n",
    "    encoded =  encoder.encode(sentences)\n",
    "    return encoded\n",
    "        \n",
    "    \n",
    "def summarize(text):\n",
    "    \"\"\"\n",
    "    Performs summarization of text\n",
    "    \"\"\"\n",
    "    summary = []\n",
    "    print('Splitting into sentences...')\n",
    "    token_text = tokenize_text(text)\n",
    "    print('Starting to encode...')\n",
    "    enc_text = skipthought_encode(token_text)\n",
    "    print('Encoding Finished')\n",
    "    n_clusters = int(np.ceil(len(enc_text)*0.3))\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    kmeans = kmeans.fit(enc_text)\n",
    "    avg = []\n",
    "    closest = []\n",
    "    for j in range(n_clusters):\n",
    "        idx = np.where(kmeans.labels_ == j)[0]\n",
    "        avg.append(np.mean(idx))\n",
    "    closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_,\\\n",
    "                                                   enc_text)\n",
    "    ordering = sorted(range(n_clusters), key=lambda k: avg[k])\n",
    "    summary = ' '.join([token_text[closest[idx]] for idx in ordering])\n",
    "    print('Clustering Finished')\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
